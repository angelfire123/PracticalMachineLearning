---
title: "Machine Learning - Course Project (Writeup)"
author: "Cristian Popescu"
date: "Thursday, October 23, 2014"
output: html_document
---

```{r echo=FALSE, warning=FALSE, message=FALSE}
library(caret)
library(rattle)
#library(mboost)
require(knitr) # required for knitting from rmd to md
require(markdown) # required for md to html 
require(klaR)
require(MASS)
require(rpart)
require(randomForest)
require("plyr")
#packages: knitr, markdown, caret, rattle, mboost, e1071, klaR, gbm, MASS, rpart, randomForest
```


###*Abstract*
*This report will focus on analysing and quantifying data on personal activities. Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement â€“ a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. The analysis will use the data from accelerometers on the belt, forearm, arm, and dumbell of all the participants.*

###Introduction

The report starts by loading the data from ```pml-training.csv``` and ```pml-testing.csv```. The ```pml-training.csv``` file will be used for training and testing, whereas the ```pml-testing.csv``` will be used for validation.


```{r}
myOriginalTraining = read.csv("pml-training.csv", header = TRUE)
myValidation = read.csv("pml-testing.csv", header = TRUE)
```

As a first observation, the data from ```pml-training.csv```, does not contain only numerical/binery data. This is a good indication on what would be the methods that we can use for training our system. We would not be able to use ```glm``` or ```lm``` models.

We will continue by extracting the right testing sample to be used. For that we will use the cross validation method ```k-fold```. We will split the ```myOriginalTraining``` data into three parts: first split of 5% (training) and 95% (training and testing). We will split again the 95% into 25% (testing) and 75% (training). We will merge the two trainings.  

```{r}
inTrain <- createDataPartition(y=myOriginalTraining$classe, p=0.95, list=FALSE)
myRestOfData <- myOriginalTraining[inTrain,]
myTraining1 <- myOriginalTraining[-inTrain,]
inTrain <- createDataPartition(y=myRestOfData$classe, p=0.25, list=FALSE)
myTesting1 <- myRestOfData[inTrain,]
inTrain <- createDataPartition(y=myRestOfData$classe, p=0.25, list=FALSE)
myTesting2 <- myRestOfData[inTrain,]
myTraining2 <- myRestOfData[-inTrain,]

#myTraining <- rbind(myTraining, myRestOfData[-inTrain,])
```

###Pre-Processing

Having the split now into training and testing, we will start to filter the original data and keep only the required predictors:
```{r}
accelIndex <- grep("^accel",names(myTraining1))
myListOfPredictors <- c(names(myTraining1)[accelIndex], "classe")
myTraining1 <- subset(myTraining1, select=myListOfPredictors)
myTesting1 <-  subset(myTesting1, select=myListOfPredictors)
myTraining2 <- subset(myTraining2, select=myListOfPredictors)
myTesting2 <-  subset(myTesting2, select=myListOfPredictors)
```

The required predictors are ```r myListOfPredictors```.

As a pre-processing, we will use the ```PCA``` method. Since, ```PCA``` method only is applicable for numeric data, we will have to *hide* the ```classe``` column. We will try to do a prediction using the new data:
```{r warning=FALSE, message=FALSE}
myNoClasseTraining <- subset(myTraining1, select=-c(classe))
preProc <- preProcess(myNoClasseTraining, method="pca", pcaComp=2)
myPrediction <- predict(preProc, myNoClasseTraining)
```

Lets take a look at the results (*black* is predicted data, whereas red if the testing data):
```{r}
plot(myPrediction[,1], myPrediction[,2], col=c("black","red"))
```

As we can see from the previous graph, we have 4 main groups (one group is actually splittled in two subgroups) identified on the screen, corresponding to the 4 main activities (belt, forearm, arm, and dumbell).

The ```PCA``` pre-processing method is not too conclusive because we are not going to use any linear method due to the nature of the data (non-numeric data). There is alot of overlapping and also distinction for each of the group for prediction and testing data.

###Building multiple models using Random Forest Trees 

Next, we will build two models using Random Forest Trees (```rf```) and different training and testing sets and will fit a models that combines both predictions. We will use the cross-validation strategy to build our final model.

```{r warning=FALSE, message=FALSE}

#building models using Random Forest models
modelFitWithTrees1 <- train(classe ~ ., data=myTraining1, method="rf", trControl=trainControl(method="cv"), number=3)
modelFitWithTrees2 <- train(classe ~ ., data=myTraining2, method="rf", trControl=trainControl(method="cv"), number=3)
```

We will take a look at the summary of the models  with both trainings and testing groups:
```{r}
print(modelFitWithTrees1)
print(modelFitWithTrees2)
```

Next, we will generate the two corresponing predictions and visualize them using:
```{r warning=FALSE, message=FALSE}
pred11 <- predict(modelFitWithTrees1, myTesting1)
pred22 <- predict(modelFitWithTrees2, myTesting2)
table(pred11, pred22)
```

We will be using now cross validation, by taking the previous prediction models and using them to the other testing sets (e.g. ```pred1``` against ```myTesting2``` and ```pred2``` against ```myTesting1```):
```{r warning=FALSE, message=FALSE}
pred12 <- predict(modelFitWithTrees1, myTesting2)
pred21 <- predict(modelFitWithTrees2, myTesting1)
table(pred12, pred21)
```


We will fit a model that combines all previous predictions and traint it using ```rf``` (*Random Forest Trees*):
```{r warning=FALSE, message=FALSE}
predDF <- data.frame(pred11, pred22, pred12, pred21, classe=myTesting1$classe)
combModFit <- train(classe ~ ., data=predDF, method="rf", trControl=trainControl(method="cv"), number=3)
combPred <- predict(combModFit, predDF)
qplot(combPred, classe, colour=classe, data=predDF)
```

Lets take a look at the model found:
```{r warning=FALSE, message=FALSE}
print(combModFit)
```

Next, we will calculate the ```confusionMatrix``` for all of the previous models:
```{r warning=FALSE, message=FALSE}
confMatrix11 <- confusionMatrix(pred11, myTesting1$classe)
confMatrix12 <- confusionMatrix(pred12, myTesting2$classe)
confMatrix21 <- confusionMatrix(pred21, myTesting1$classe)
confMatrix22 <- confusionMatrix(pred22, myTesting2$classe)
confMatrixComb <- confusionMatrix(combPred, predDF$classe)
```

The estimation of the error is (for each of the predictions):
1. for model ```modelFitWithTrees1``` with testing ```myTesting1```: ```r 1-confMatrix11$overall[1]```;
2. for model ```modelFitWithTrees1``` with testing ```myTesting2```: ```r 1-confMatrix12$overall[1]```;
3. for model ```modelFitWithTrees2``` with testing ```myTesting1```: ```r 1-confMatrix21$overall[1]```;
4. for model ```modelFitWithTrees2``` with testing ```myTesting2```: ```r 1-confMatrix22$overall[1]```;
5. for combined model ```combModFit``` with testing the combined prediction (of the previous models) ```predDF```: ```r 1-confMatrixComb$overall[1]```;


### Validation with ```pml-testing.csv```

Based on the previous models, the best result on accurancy is from ```combPred``` model (```Accuracy : ``` ```r confMatrixComb$overall[1]  ```). We will use the combined model in order to apply the validation data from ```pml-testing.csv``` file.

```{r warning=FALSE, message=FALSE}
predValidation <- predict(combModFit, myValidation)
```

Next, we will visualize the computed validation:
```{r warning=FALSE, message=FALSE}
predValidationOccurances <-count(predValidation)
print(predValidationOccurances)
qplot(predValidationOccurances$x, predValidationOccurances$freq, colour = predValidationOccurances$x)
```
